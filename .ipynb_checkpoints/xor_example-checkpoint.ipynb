{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR operation by Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.706317 [[ 0.30787715 -0.88231969]\n",
      " [ 0.24494746  0.10857227]] [[-0.70726705]\n",
      " [ 0.17905134]]\n",
      "1000 0.693119 [[ 0.17551419 -0.89084482]\n",
      " [ 0.16362746  0.20531729]] [[-0.55442113]\n",
      " [ 0.22783217]]\n",
      "2000 0.692897 [[ 0.10038898 -0.93153638]\n",
      " [ 0.14226653  0.29082319]] [[-0.5463478 ]\n",
      " [ 0.25428766]]\n",
      "3000 0.692361 [[ 0.03192817 -1.01877558]\n",
      " [ 0.14243136  0.40855339]] [[-0.55464786]\n",
      " [ 0.34971717]]\n",
      "4000 0.689531 [[-0.05926506 -1.2459048 ]\n",
      " [ 0.17179145  0.65225321]] [[-0.58910418]\n",
      " [ 0.60321683]]\n",
      "5000 0.658608 [[-0.27824572 -2.04368091]\n",
      " [ 0.32805979  1.48052585]] [[-0.74305999]\n",
      " [ 1.49309599]]\n",
      "6000 0.477533 [[-1.30969334 -3.76084208]\n",
      " [ 1.5397594   3.34518075]] [[-1.94688594]\n",
      " [ 3.65180969]]\n",
      "7000 0.151263 [[-3.5321517  -4.92284727]\n",
      " [ 3.84358191  4.71938753]] [[-4.93999338]\n",
      " [ 5.91134548]]\n",
      "8000 0.0673328 [[-4.48427534 -5.44795656]\n",
      " [ 4.7826376   5.28941727]] [[-6.58765364]\n",
      " [ 7.26631641]]\n",
      "[array([[ 0.06438738],\n",
      "       [ 0.94873834],\n",
      "       [ 0.90867531],\n",
      "       [ 0.05293511]], dtype=float32), array([[ 0.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 0.]], dtype=float32), array([[ True],\n",
      "       [ True],\n",
      "       [ True],\n",
      "       [ True]], dtype=bool), 1.0]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Implement code for Sung Kim's TF lecture. See https://www.youtube.com/watch?v=9i7FBbcZPMA&feature=youtu.be\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "xy = np.loadtxt('xor_dataset.txt', unpack=True)\n",
    "\n",
    "# Need to change data structure. THESE LINES ARE DIFFERNT FROM Video BUT IT MAKES THIS CODE WORKS!\n",
    "x_data = np.transpose( xy[0:-1] )\n",
    "y_data = np.reshape( xy[-1], (4,1) )\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_uniform( [2,2], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform( [2,1], -1.0, 1.0))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([2]), name=\"Bias1\")\n",
    "b2 = tf.Variable(tf.zeros([1]), name=\"Bias2\")\n",
    "\n",
    "# Hypotheses \n",
    "L2 =  tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "hypothesis = tf.sigmoid( tf.matmul(L2,W2) + b2)\n",
    "\n",
    "# Cost function \n",
    "cost = -tf.reduce_mean( Y*tf.log(hypothesis)+(1-Y)* tf.log(1.-hypothesis) )\n",
    "\n",
    "# Minimize cost.\n",
    "a = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Initializa all variables.\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(8001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            print(\n",
    "                step, \n",
    "                sess.run(cost, feed_dict={X:x_data, Y:y_data}), \n",
    "                sess.run(W1),\n",
    "                sess.run(W2)\n",
    "            )\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal( tf.floor(hypothesis+0.5), Y)\n",
    "    accuracy = tf.reduce_mean(tf.cast( correct_prediction, \"float\" ) )\n",
    "    \n",
    "    # Check accuracy\n",
    "    print( sess.run( [hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy], \n",
    "                   feed_dict={X:x_data, Y:y_data}) )\n",
    "    print( \"Accuracy:\", accuracy.eval({X:x_data, Y:y_data}) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use 'wide' neural network to solve XOR problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.939016 [[-0.85606247  0.73059428  0.77462631 -0.3896639  -0.08149331  0.76219189\n",
      "   0.05174674  0.89194518  0.53705561 -0.01290749]\n",
      " [-0.56907946 -0.83164084 -0.01942129  0.89718544  0.79482955  0.99284232\n",
      "  -0.25736266 -0.59783733  0.85438913 -0.63163215]] [[-0.55895883]\n",
      " [-0.8063069 ]\n",
      " [-0.86987585]\n",
      " [-0.02713636]\n",
      " [-0.48638582]\n",
      " [ 0.15208146]\n",
      " [ 0.37765968]\n",
      " [-0.22403061]\n",
      " [-0.60254139]\n",
      " [ 0.30870259]]\n",
      "1000 0.615363 [[-1.633129    1.15083015  0.86933434 -0.37121201 -0.18196048  1.82052016\n",
      "   0.02581124  0.93192804  0.30806234  0.07533117]\n",
      " [-1.46620035 -0.65160596 -0.07657609  0.91693616  0.95575196  1.8603729\n",
      "  -0.40390274 -0.57092583  0.77949011 -0.77410185]] [[-1.35776865]\n",
      " [-0.82888961]\n",
      " [-0.81315273]\n",
      " [-0.2029199 ]\n",
      " [-0.61001915]\n",
      " [ 1.61133587]\n",
      " [ 0.57980025]\n",
      " [-0.14876781]\n",
      " [-0.46170142]\n",
      " [ 0.60113639]]\n",
      "2000 0.350253 [[-2.89773512  2.51311016  1.40825009 -0.27757058 -0.27283347  3.48000145\n",
      "  -0.22738236  1.21727538  0.38566223  0.02805942]\n",
      " [-2.80407739 -1.26434338  0.00937189  1.33914614  1.65956652  3.43731904\n",
      "  -0.90580338 -0.41532332  1.07951915 -1.39313006]] [[-2.79189396]\n",
      " [-2.05276752]\n",
      " [-1.32108951]\n",
      " [-0.78170228]\n",
      " [-1.3442148 ]\n",
      " [ 3.94571495]\n",
      " [ 1.23699033]\n",
      " [-0.46331537]\n",
      " [-1.08756936]\n",
      " [ 1.48586345]]\n",
      "3000 0.131372 [[-3.65614796  3.91144681  1.87225032 -0.35171506 -0.57962841  4.47095776\n",
      "  -0.42289427  1.5292083   0.62112564  0.10258979]\n",
      " [-3.58385658 -2.30658245  0.02972294  1.83948123  2.46575689  4.42327213\n",
      "  -1.37344193 -0.36845571  1.42220199 -2.13878894]] [[-3.81642413]\n",
      " [-3.71753311]\n",
      " [-1.81028128]\n",
      " [-1.28512514]\n",
      " [-2.09416151]\n",
      " [ 5.81847763]\n",
      " [ 2.15841532]\n",
      " [-0.75283748]\n",
      " [-1.74082124]\n",
      " [ 2.56336522]]\n",
      "4000 0.0609246 [[-4.02408314  4.58494806  2.09441042 -0.45993873 -0.85695392  4.92190409\n",
      "  -0.64135724  1.69580233  0.84802705  0.17713632]\n",
      " [-3.96295619 -2.92269015  0.04890624  2.12124109  2.93345475  4.87904644\n",
      "  -1.58270204 -0.39966959  1.58334076 -2.55875874]] [[-4.41659594]\n",
      " [-4.77191496]\n",
      " [-2.06248093]\n",
      " [-1.54388893]\n",
      " [-2.53076768]\n",
      " [ 6.84100723]\n",
      " [ 2.72025585]\n",
      " [-0.89944077]\n",
      " [-2.14345503]\n",
      " [ 3.1444633 ]]\n",
      "5000 0.0357255 [[-4.23422956  4.93436384  2.20593214 -0.54523468 -1.06426394  5.16782331\n",
      "  -0.83768064  1.79204178  1.01960933  0.24802803]\n",
      " [-4.17596483 -3.27633619  0.07285351  2.29398227  3.21249604  5.12469959\n",
      "  -1.69829369 -0.4302116   1.68431914 -2.81172132]] [[-4.78705883]\n",
      " [-5.40315247]\n",
      " [-2.19810891]\n",
      " [-1.70094919]\n",
      " [-2.8128252 ]\n",
      " [ 7.44240475]\n",
      " [ 3.08438993]\n",
      " [-0.97852457]\n",
      " [-2.41036892]\n",
      " [ 3.48016167]]\n",
      "6000 0.0240728 [[-4.37271404  5.14996195  2.27067184 -0.61184287 -1.22114241  5.32545233\n",
      "  -0.99904555  1.85579121  1.14915991  0.31271011]\n",
      " [-4.31492043 -3.50641131  0.09515373  2.4131701   3.39867282  5.28095722\n",
      "  -1.780334   -0.45478994  1.75918221 -2.98520541]] [[-5.0420332 ]\n",
      " [-5.82482243]\n",
      " [-2.28261924]\n",
      " [-1.80950081]\n",
      " [-3.01492953]\n",
      " [ 7.84563541]\n",
      " [ 3.34821534]\n",
      " [-1.02817559]\n",
      " [-2.60407639]\n",
      " [ 3.70312095]]\n",
      "7000 0.0176719 [[-4.47273827  5.29912519  2.31264687 -0.66542441 -1.3443017   5.43727112\n",
      "  -1.13121724  1.90216112  1.25079477  0.37066814]\n",
      " [-4.41489744 -3.67077589  0.11486268  2.50201893  3.53355074  5.39153481\n",
      "  -1.8457936  -0.47485563  1.81848645 -3.11436629]] [[-5.23175764]\n",
      " [-6.13184404]\n",
      " [-2.34131002]\n",
      " [-1.89040232]\n",
      " [-3.16938853]\n",
      " [ 8.14126682]\n",
      " [ 3.55327868]\n",
      " [-1.0628866 ]\n",
      " [-2.75372791]\n",
      " [ 3.8655479 ]]\n",
      "8000 0.0137316 [[-4.54952097  5.41030931  2.3420558  -0.70979142 -1.4442929   5.52202606\n",
      "  -1.24117267  1.93806851  1.3334415   0.42256647]\n",
      " [-4.49161243 -3.7959168   0.13226345  2.5718143   3.63707542  5.47539377\n",
      "  -1.90085053 -0.49169287  1.86730194 -3.21585727]] [[-5.38068533]\n",
      " [-6.36914825]\n",
      " [-2.3852272 ]\n",
      " [-1.95383751]\n",
      " [-3.29283857]\n",
      " [ 8.3712616 ]\n",
      " [ 3.72015548]\n",
      " [-1.0890044 ]\n",
      " [-2.87458992]\n",
      " [ 3.99136496]]\n",
      "[array([[ 0.00816902],\n",
      "       [ 0.98628181],\n",
      "       [ 0.98552203],\n",
      "       [ 0.01816002]], dtype=float32), array([[ 0.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 0.]], dtype=float32), array([[ True],\n",
      "       [ True],\n",
      "       [ True],\n",
      "       [ True]], dtype=bool), 1.0]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Use 'wide' neural network (10 neurals) to solve XOR problem. \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "xy = np.loadtxt('xor_dataset.txt', unpack=True)\n",
    "\n",
    "x_data = np.transpose( xy[0:-1] )\n",
    "y_data = np.reshape( xy[-1], (4,1) )\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Wide network: Use more neurons in each layer. \n",
    "W1 = tf.Variable(tf.random_uniform( [2,10], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform( [10,1], -1.0, 1.0))\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([10]), name=\"Bias1\")\n",
    "b2 = tf.Variable(tf.zeros([1]), name=\"Bias2\")\n",
    "\n",
    "# Hypotheses \n",
    "L2 =  tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "hypothesis = tf.sigmoid( tf.matmul(L2,W2) + b2)\n",
    "\n",
    "# Cost function \n",
    "cost = -tf.reduce_mean( Y*tf.log(hypothesis)+(1-Y)* tf.log(1.-hypothesis) )\n",
    "\n",
    "# Minimize cost.\n",
    "a = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Initializa all variables.\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(8001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            print(\n",
    "                step, \n",
    "                sess.run(cost, feed_dict={X:x_data, Y:y_data}), \n",
    "                sess.run(W1),\n",
    "                sess.run(W2)\n",
    "            )\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal( tf.floor(hypothesis+0.5), Y)\n",
    "    accuracy = tf.reduce_mean(tf.cast( correct_prediction, \"float\" ) )\n",
    "    \n",
    "    # Check accuracy\n",
    "    print( sess.run( [hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy], \n",
    "                   feed_dict={X:x_data, Y:y_data}) )\n",
    "    print( \"Accuracy:\", accuracy.eval({X:x_data, Y:y_data}) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use 'Deep' neural network to solve XOR problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.695899 [[ 0.05729413  0.51451641  0.88735789  0.96140552  0.45755371]\n",
      " [ 0.02158975  0.5340535  -0.56092513  0.18043032  0.22230092]] [[ 0.99723405 -0.26517531  0.50503188  0.02051228]\n",
      " [-0.72842687 -0.70609009  0.48214823 -0.31720841]\n",
      " [-0.68178713  0.82693774 -0.12611331 -0.58795184]\n",
      " [ 0.24516402  0.10893862  0.99789387 -0.35805094]\n",
      " [ 0.96453381 -0.70980263 -0.69242996  0.24875957]]\n",
      "1000 0.691304 [[ 0.06990006  0.6350373   0.78995168  1.02602732  0.43388036]\n",
      " [ 0.04209872  0.68840468 -0.60211295  0.41251904  0.12467673]] [[ 0.99577379 -0.26568487  0.50766349  0.01983534]\n",
      " [-0.75742519 -0.69680834  0.54632872 -0.38128623]\n",
      " [-0.67263836  0.8264963  -0.14617382 -0.53756011]\n",
      " [ 0.22369672  0.11759883  1.04786527 -0.39293882]\n",
      " [ 0.96345425 -0.70863628 -0.67868471  0.24622309]]\n",
      "2000 0.686552 [[ 0.10008515  0.88405704  0.71933472  1.190521    0.4265337 ]\n",
      " [ 0.07315027  0.95250857 -0.69873559  0.74583024  0.05188567]] [[ 0.99884403 -0.26667249  0.4875966   0.03821776]\n",
      " [-0.82187897 -0.68687475  0.65723449 -0.49049586]\n",
      " [-0.68139756  0.82878184 -0.15850365 -0.50333256]\n",
      " [ 0.15926768  0.12832373  1.16056323 -0.49175197]\n",
      " [ 0.97159261 -0.70912194 -0.68773371  0.27082726]]\n",
      "3000 0.664764 [[ 0.174842    1.42598557  0.66645294  1.56915212  0.45739976]\n",
      " [ 0.12867412  1.48829317 -0.87518704  1.30113101  0.00187175]] [[ 1.01441383 -0.26642606  0.38841167  0.11173794]\n",
      " [-1.03079462 -0.68999141  0.88170582 -0.74056119]\n",
      " [-0.73291898  0.8279193  -0.19247983 -0.47118899]\n",
      " [-0.04375887  0.1254537   1.39232159 -0.73331326]\n",
      " [ 0.99962735 -0.70883065 -0.77655655  0.35962144]]\n",
      "4000 0.553394 [[ 0.48661998  2.54236555  0.61530614  2.45681143  0.82900995]\n",
      " [ 0.36158511  2.56939006 -1.33356249  2.33107352  0.11301913]] [[ 1.23312294 -0.26211697  0.08829018  0.42270833]\n",
      " [-1.51869285 -0.75064313  1.42327106 -1.23910081]\n",
      " [-0.96520573  0.80365652 -0.15864086 -0.53079373]\n",
      " [-0.4598527   0.06967431  1.89414954 -1.16932416]\n",
      " [ 1.25618625 -0.70456022 -1.08469248  0.71608865]]\n",
      "5000 0.169686 [[ 1.18313587  3.8056488   1.33327055  3.55698943  2.11737752]\n",
      " [ 1.0052464   3.76944733 -2.87872148  3.44048357  0.13848193]] [[ 2.21157527 -0.25629643 -0.88256121  1.45460522]\n",
      " [-2.22568965 -0.72445774  1.90954006 -1.77667022]\n",
      " [-2.0597415   0.81993091  0.77433479 -1.51024187]\n",
      " [-1.02717161  0.09516104  2.26397729 -1.5781883 ]\n",
      " [ 2.27916789 -0.69582689 -1.99788272  1.78751838]]\n",
      "6000 0.0301732 [[ 1.53324938  4.26678514  2.11369324  3.97008729  2.76242423]\n",
      " [ 1.39022315  4.23377275 -3.69876933  3.86773777 -0.13418205]] [[ 2.70435047 -0.24818836 -1.54837465  1.96355307]\n",
      " [-2.64013982 -0.61322284  2.23891711 -2.14389801]\n",
      " [-2.57174921  0.88839376  1.62892044 -2.07490206]\n",
      " [-1.37353849  0.20110096  2.53250122 -1.88614309]\n",
      " [ 2.77554417 -0.67050189 -2.51910734  2.26981759]]\n",
      "7000 0.0129322 [[ 1.6974653   4.41213512  2.39462829  4.10198069  3.01289463]\n",
      " [ 1.55827987  4.38556004 -3.95588207  4.00941324 -0.31360668]] [[ 2.86844325 -0.24873799 -1.80054641  2.1469028 ]\n",
      " [-2.78935218 -0.55842763  2.36166763 -2.27267623]\n",
      " [-2.74877453  0.92480409  1.94258559 -2.2753191 ]\n",
      " [-1.50117505  0.25358024  2.63727832 -1.99697578]\n",
      " [ 2.93536234 -0.65896767 -2.69695401  2.43277931]]\n",
      "8000 0.00770814 [[ 1.79603696  4.48709249  2.54286432  4.17059469  3.15553164]\n",
      " [ 1.65652788  4.46482038 -4.08986282  4.08416605 -0.43253255]] [[ 2.9577446  -0.25082198 -1.94343734  2.24980068]\n",
      " [-2.86969066 -0.52449274  2.42787719 -2.3416841 ]\n",
      " [-2.84553838  0.94817883  2.11308718 -2.38505244]\n",
      " [-1.57062495  0.28614748  2.69457579 -2.0570147 ]\n",
      " [ 3.02197981 -0.65279138 -2.79543757  2.52265573]]\n",
      "9000 0.00534692 [[ 1.86483192  4.53522348  2.6388483   4.21489477  3.25217581]\n",
      " [ 1.7240715   4.51608467 -4.17672348  4.1328392  -0.51941431]] [[ 3.01744938 -0.25320533 -2.0406661   2.31969976]\n",
      " [-2.92243099 -0.50025076  2.47150922 -2.38699174]\n",
      " [-2.90995955  0.96524549  2.22577381 -2.45812011]\n",
      " [-1.61648607  0.30943295  2.73261428 -2.09667206]\n",
      " [ 3.07995987 -0.64902681 -2.86200094  2.58332944]]\n",
      "10000 0.00403863 [[ 1.91706705  4.56983709  2.70816636  4.24687052  3.32407403]\n",
      " [ 1.77487242  4.55313396 -4.23967743  4.16818094 -0.58715975]] [[ 3.06174922 -0.25559175 -2.11348271  2.3720808 ]\n",
      " [-2.96088862 -0.48150903  2.50345993 -2.42008162]\n",
      " [-2.95748639  0.97864902  2.30836773 -2.51202393]\n",
      " [-1.65005875  0.32744265  2.76060677 -2.12575293]\n",
      " [ 3.12305784 -0.64656454 -2.91174507  2.62866306]]\n",
      "11000 0.00321968 [[ 1.95888293  4.59648943  2.76167059  4.27156353  3.38075686]\n",
      " [ 1.81528199  4.58176041 -4.28846216  4.19558668 -0.64235818]] [[ 3.09672809 -0.2578986  -2.17129278  2.41372442]\n",
      " [-2.99078941 -0.46628672  2.52840614 -2.44586062]\n",
      " [-2.99480247  0.98967171  2.37285209 -2.55434465]\n",
      " [-1.67623734  0.34207293  2.78254223 -2.14847493]\n",
      " [ 3.15714502 -0.64489144 -2.95122004  2.66464686]]\n",
      "12000 0.00266373 [[ 1.99359083  4.61796236  2.80485296  4.29150629  3.42723536]\n",
      " [ 1.84867275  4.60488176 -4.32798004  4.21779537 -0.68875915]] [[ 3.12550902 -0.26010323 -2.21902037  2.44816208]\n",
      " [-3.0150609  -0.45350045  2.548733   -2.46682978]\n",
      " [-3.02534366  0.9990263   2.42536879 -2.58898139]\n",
      " [-1.69753277  0.3543627   2.80046439 -2.16699791]\n",
      " [ 3.18523002 -0.64373481 -2.98381519  2.69437456]]\n",
      "13000 0.00226397 [[ 2.02316284  4.63584042  2.84083104  4.30813599  3.4664433 ]\n",
      " [ 1.87703037  4.62416887 -4.36101389  4.23635769 -0.72867805]] [[ 3.14988804 -0.26220399 -2.25953937  2.47744799]\n",
      " [-3.03537703 -0.44249597  2.56580687 -2.48441887]\n",
      " [-3.05109382  1.00714827  2.46945262 -2.61818242]\n",
      " [-1.71538997  0.36493969  2.81555486 -2.18256593]\n",
      " [ 3.20904994 -0.64293486 -3.01150012  2.71963906]]\n",
      "14000 0.00196389 [[ 2.04886508  4.65107203  2.87152886  4.32234192  3.50023437]\n",
      " [ 1.90161574  4.64063501 -4.38928461  4.25224304 -0.76363724]] [[ 3.17099428 -0.26420623 -2.29466629  2.50288248]\n",
      " [-3.05277419 -0.43284935  2.58048177 -2.49951553]\n",
      " [-3.07328963  1.01432431  2.50730324 -2.64335489]\n",
      " [-1.73070812  0.3742111   2.82854509 -2.1959486 ]\n",
      " [ 3.22968793 -0.64239067 -3.03551626  2.74156713]]\n",
      "15000 0.00173107 [[ 2.07155418  4.66429663  2.89820743  4.33469248  3.529845  ]\n",
      " [ 1.92327774  4.65495729 -4.41392851  4.26608801 -0.79468817]] [[ 3.18957782 -0.26611581 -2.32561922  2.52533245]\n",
      " [-3.06794667 -0.42427057  2.59331775 -2.5127058 ]\n",
      " [-3.09275532  1.02075088  2.54037833 -2.66542578]\n",
      " [-1.74408197  0.38245535  2.83992648 -2.20765376]\n",
      " [ 3.2478714  -0.64203769 -3.05669212  2.76091862]]\n",
      "16000 0.00154552 [[ 2.09183264  4.67595959  2.921736    4.34560728  3.556144  ]\n",
      " [ 1.94261074  4.66759443 -4.43570375  4.2783227  -0.82258469]] [[ 3.20615387 -0.26794019 -2.35324907  2.54540491]\n",
      " [-3.08136439 -0.41655275  2.6046977  -2.52439642]\n",
      " [-3.11006117  1.02657115  2.56968641 -2.68504858]\n",
      " [-1.75592411  0.38987163  2.85003638 -2.21803999]\n",
      " [ 3.26410675 -0.64183021 -3.07560682  2.77820945]]\n",
      "17000 0.00139447 [[ 2.11014175  4.68637609  2.94273949  4.35533905  3.57975698]\n",
      " [ 1.96004856  4.67890072 -4.45519257  4.28926611 -0.84788567]] [[ 3.22110748 -0.26968923 -2.37817836  2.5635469 ]\n",
      " [-3.09337187 -0.40954319  2.61491132 -2.53487325]\n",
      " [-3.12561536  1.03188503  2.59595585 -2.70269132]\n",
      " [-1.7665323   0.3966063   2.85911679 -2.2273581 ]\n",
      " [ 3.27875471 -0.64173853 -3.09268332  2.79383421]]\n",
      "18000 0.00126929 [[ 2.12681723  4.69575167  2.96167064  4.3641367   3.60115671]\n",
      " [ 1.97591484  4.68907309 -4.47279263  4.29914951 -0.87101603]] [[ 3.23471761 -0.27136561 -2.40086389  2.58007765]\n",
      " [-3.10422254 -0.40312606  2.62415695 -2.54435277]\n",
      " [-3.13973522  1.03677964  2.61972237 -2.7186954 ]\n",
      " [-1.77612329  0.40277109  2.86734891 -2.23580194]\n",
      " [ 3.2920897  -0.64173013 -3.10823655  2.80806589]]\n",
      "19000 0.00116394 [[ 2.14212012  4.70426178  2.97887635  4.37209463  3.62069869]\n",
      " [ 1.9904598   4.69834566 -4.48882341  4.30813694 -0.89230365]] [[ 3.24719214 -0.27297807 -2.42166853  2.59525681]\n",
      " [-3.11410403 -0.39721221  2.63260031 -2.55300617]\n",
      " [-3.15263748  1.04130983  2.64140034 -2.73333192]\n",
      " [-1.78486609  0.4084518   2.87487435 -2.24350762]\n",
      " [ 3.30431557 -0.64179808 -3.12250566  2.82113886]]\n",
      "20000 0.00107414 [[ 2.15623403  4.71207905  2.99463367  4.37942791  3.63865972]\n",
      " [ 2.0038743   4.70679474 -4.50354099  4.31641388 -0.91201073]] [[ 3.25871491 -0.27453151 -2.44086909  2.60928845]\n",
      " [-3.12317061 -0.39173007  2.64036512 -2.56095004]\n",
      " [-3.16451859  1.04552817  2.66130853 -2.74680781]\n",
      " [-1.79289305  0.41371682  2.88179588 -2.25060487]\n",
      " [ 3.31561017 -0.64191836 -3.13567519  2.83321047]]\n",
      "[array([[  6.77078729e-04],\n",
      "       [  9.98777092e-01],\n",
      "       [  9.98867750e-01],\n",
      "       [  1.26195909e-03]], dtype=float32), array([[ 0.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 0.]], dtype=float32), array([[ True],\n",
      "       [ True],\n",
      "       [ True],\n",
      "       [ True]], dtype=bool), 1.0]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Use 'Deep' neural network to solve XOR problem. \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "xy = np.loadtxt('xor_dataset.txt', unpack=True)\n",
    "\n",
    "x_data = np.transpose( xy[0:-1] )\n",
    "y_data = np.reshape( xy[-1], (4,1) )\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Deep network configuration.: Use more layers. \n",
    "W1 = tf.Variable(tf.random_uniform( [2,5], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform( [5,4], -1.0, 1.0))\n",
    "W3 = tf.Variable(tf.random_uniform( [4,1], -1.0, 1.0))\n",
    "\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([5]), name=\"Bias1\")\n",
    "b2 = tf.Variable(tf.zeros([4]), name=\"Bias2\")\n",
    "b3 = tf.Variable(tf.zeros([1]), name=\"Bias3\")\n",
    "\n",
    "\n",
    "# Hypotheses \n",
    "L2 =  tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "L3 =  tf.sigmoid(tf.matmul(L2,W2)+b2)\n",
    "hypothesis = tf.sigmoid( tf.matmul(L3,W3) + b3)\n",
    "\n",
    "# Cost function \n",
    "cost = -tf.reduce_mean( Y*tf.log(hypothesis)+(1-Y)* tf.log(1.-hypothesis) )\n",
    "\n",
    "# Minimize cost.\n",
    "a = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Initializa all variables.\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(20001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            print(\n",
    "                step, \n",
    "                sess.run(cost, feed_dict={X:x_data, Y:y_data}), \n",
    "                sess.run(W1),\n",
    "                sess.run(W2)\n",
    "            )\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal( tf.floor(hypothesis+0.5), Y)\n",
    "    accuracy = tf.reduce_mean(tf.cast( correct_prediction, \"float\" ) )\n",
    "    \n",
    "    # Check accuracy\n",
    "    print( sess.run( [hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy], \n",
    "                   feed_dict={X:x_data, Y:y_data}) )\n",
    "    print( \"Accuracy:\", accuracy.eval({X:x_data, Y:y_data}) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Use Xavier initializer. \n",
    "\n",
    "def xavier_init(n_inputs, n_outputs, uniform=True):\n",
    "  \"\"\"Set the parameter initialization using the method described.\n",
    "  This method is designed to keep the scale of the gradients roughly the same\n",
    "  in all layers.\n",
    "  Xavier Glorot and Yoshua Bengio (2010):\n",
    "           Understanding the difficulty of training deep feedforward neural\n",
    "           networks. International conference on artificial intelligence and\n",
    "           statistics.\n",
    "  Args:\n",
    "    n_inputs: The number of input nodes into each output.\n",
    "    n_outputs: The number of output nodes for each input.\n",
    "    uniform: If true use a uniform distribution, otherwise use a normal.\n",
    "  Returns:\n",
    "    An initializer.\n",
    "  \"\"\"\n",
    "  if uniform:\n",
    "    # 6 was used in the paper.\n",
    "    init_range = math.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "    return tf.random_uniform_initializer(-init_range, init_range)\n",
    "  else:\n",
    "    # 3 gives us approximately the same limits as above since this repicks\n",
    "    # values greater than 2 standard deviations from the mean.\n",
    "    stddev = math.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "    return tf.truncated_normal_initializer(stddev=stddev)\n",
    "\n",
    "\n",
    "# Use 'Deep' neural network to solve XOR problem. \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "xy = np.loadtxt('xor_dataset.txt', unpack=True)\n",
    "\n",
    "x_data = np.transpose( xy[0:-1] )\n",
    "y_data = np.reshape( xy[-1], (4,1) )\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Deep network configuration.: Use more layers. \n",
    "W1 = tf.Variable(tf.random_uniform( [2,5], -1.0, 1.0))\n",
    "W2 = tf.Variable(tf.random_uniform( [5,4], -1.0, 1.0))\n",
    "W3 = tf.Variable(tf.random_uniform( [4,1], -1.0, 1.0))\n",
    "\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([5]), name=\"Bias1\")\n",
    "b2 = tf.Variable(tf.zeros([4]), name=\"Bias2\")\n",
    "b3 = tf.Variable(tf.zeros([1]), name=\"Bias3\")\n",
    "\n",
    "\n",
    "# Hypotheses \n",
    "L2 =  tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "L3 =  tf.sigmoid(tf.matmul(L2,W2)+b2)\n",
    "hypothesis = tf.sigmoid( tf.matmul(L3,W3) + b3)\n",
    "\n",
    "# Cost function \n",
    "cost = -tf.reduce_mean( Y*tf.log(hypothesis)+(1-Y)* tf.log(1.-hypothesis) )\n",
    "\n",
    "# Minimize cost.\n",
    "a = tf.Variable(0.1)\n",
    "optimizer = tf.train.GradientDescentOptimizer(a)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# Initializa all variables.\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(20001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            print(\n",
    "                step, \n",
    "                sess.run(cost, feed_dict={X:x_data, Y:y_data}), \n",
    "                sess.run(W1),\n",
    "                sess.run(W2)\n",
    "            )\n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal( tf.floor(hypothesis+0.5), Y)\n",
    "    accuracy = tf.reduce_mean(tf.cast( correct_prediction, \"float\" ) )\n",
    "    \n",
    "    # Check accuracy\n",
    "    print( sess.run( [hypothesis, tf.floor(hypothesis+0.5), correct_prediction, accuracy], \n",
    "                   feed_dict={X:x_data, Y:y_data}) )\n",
    "    print( \"Accuracy:\", accuracy.eval({X:x_data, Y:y_data}) )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
